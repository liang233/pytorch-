代码见这个https://github.com/JiahuiYu/wdsr_ntire2018/blob/master/wdsr_a.py
先看看简单的


37行
wn = lambda x: torch.nn.utils.weight_norm(x) 简单定义一个函数

39到40行 self.rgb_mean = torch.autograd.Variable(torch.FloatTensor(
[args.r_mean, args.g_mean, args.b_mean])).view([1, 3, 1, 1])
1.torch.autograd.Variable：
torch.autograd.Variable用来包裹张量并记录应用的操作。
Variable可以看作是对Tensor对象周围的一个薄包装，也包含了和张量相关的梯度，以及对创建它的函数的引用。 此引用允许对创建数据的整个操作链进行回溯。
需要BP的网络都是通过Variable来计算的。如果Variable是由用户创建的，则其grad_fn将为None，我们将这些对象称为叶子Variable。 
2.view
其作用在于返回和原tensor数据个数相同，但size不同的tensor
view函数只能由于contiguous的张量上，具体而言，就是在内存中连续存储的张量。
所以，应该提前做tensor.contiguous()的操作

58行
tail.append(nn.PixelShuffle(scale))
nn.PixelShuffle(scale)    类似上采样的操作，都是将一个较多通道的特征变成较少通道的特征

67行
pad.append(torch.nn.ReplicationPad2d(5//2))
torch.nn.ReplicationPad2d  使用输入边界的复制填充输入张量

这份代码比较简单，是nitre2018 超分辨优胜代码，不同的好像也就把wn替换bn，原因不知道
